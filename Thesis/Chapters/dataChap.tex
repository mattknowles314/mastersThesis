\chapter{Data}

\epigraph{One person's data is another person's noise}{K.C. Cole}

The purpose of this small chapter is to give an overview of the data that is used for this project.

\section{Data Origin}
The primary source of data for carrying out this project was downloaded from ``cricsheet''\footnote{https://cricsheet.org/}
and stored locally on a private server. In total there are 2167 individual matches of data. Each in a JSON format, 
covering matches ranging from the $3^{rd}$ of January 2004 to the $20^{th}$ of July 2021. \\

\section{Attributes}
Each JSON file contains a considerable amount of metadata surrounding the match in question, along with 
ball-by-ball data for the entire match. We have access to attributes such as the date, where the match was played,
the entire teamsheet for both teams, who the officials were, who won- and by what margin, who won the toss; and many others.

We also have the ball-by-ball data. So for every ball bowled, it gives who were the striking and non-striking batsmen, how many runs
were scored and how. It also details if a wicket was taken that ball, and how. \\

There is more data in these files than we actually need, certainly in the case of the meta-data. If we were building a win classifier then this 
extra data would be useful, but since we are building a model to predict scores, it is largely irrelevant.

\section{Pre-Processing}
In order to clean data and make it usable, Python scripts were written to take the original JSON files and turn them into CSV files on which analysis could 
be performed in R. These scripts made heavy use of the base-Python libraries \verb|JSON| and \verb|CSV|.

\section{Problems and Future Improvements to Data Wrangling}
When it comes to machine learning, the more data the better is a general rule. This can sometimes lead to sub-problems, such as ``overfitting''; which is where the model 
learns to correspond too well to a training dataset, and fails to correlate with unseen data. Even so,  
it is much better to have as much data as possible. We will be training a neural network on 1435 data points, which strictly speaking, isn't a lot of data, but there
isn't much that can be done about this due to the cricketing calender only having a certain number of limited-overs matches each year as previously mentioned. \\

There is actually an R package called \verb|cricketdata| that allows for the direct import of data from ESPNCricinfo\footnote{https://www.epsncricinfo.com/}
and Cricsheet. However this package was published in February 2022, several months after the bulk of work on this project started. In hindsight, using this package 
would have been a lot easier and saved a considerable amount of time. With that in mind, future work on the \verb|CricNet| package (See Appendix C)
will incorperate the \verb|cricketdata| package in order to streamline the analysis pipeline as a whole. 
