\chapter{Exploratory Data Analysis}

\epigraph{Cricket often leaves you scratching your head}{James Anderson}

The purpose of this chapter is to explore the obtained dataset in more detail, which is necessary for carrying out the work in later chapters. In general, and no less
in the world of statistics, making assumptions is dangerous, and so in order to make the assumptions we do in later chapters, we must have the evidence to back it up.
This chapter not only provides that evidence, but allows us to become more familiar with our dataset, and see how modern data fits in with the previous work done in this 
field. \\
We begin by looking at the probability densities in the Fall of Wicket (FOW) variables. FOW is key in the DLS method, so we do this to get an idea of what lies underneath the
surface. We are able to explain the way FOW distributions are shaped based on the way cricket games unfold. This consistency allows us to make an assumption about Run Rates
later in Section 4.4. \\
As with any dataset, there are outliers, and the number of such will have influence on the error function that we use in later models. For that reason, we give brief discussion 
to this, before going on to look at whether or not scores in games of cricket are normally distributed around some mean. \\
Run Rates are the discussed in more detail, looking at whether or not the run rates in certain periods of the game also follow a normal distribution. Finding this out is imperative
for using Monte-Carlo simulation later on in the paper.

\section{Fall of Wicket Densities}

In this chapter, we are looking only at the first innings of the games, and only those games in which the full 50 overs were played. The 
reason for this is the models we will build are going to try and predict a score as if a full innings has been played. We begin our exploration of the data with a look at how the density of the runs scored per fall of wicket changes. This has been done for each
individual team in the dataset, and in Figures~\ref{ovrdens1fow}-\ref{ovrdens9fow}, we can see how this evolves.  

\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[scale=0.28]{figures/fow1density.png}
        \caption{Density of all teams for first wicket falling}
        \label{alldens1fow}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[scale=0.28]{figures/fow1densFull.png}
        \caption{Overall density plot for FOW 1}
        \label{ovrdens1fow}
    \end{minipage}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[scale=0.28]{figures/fow1density.png}
        \caption{Density of all teams for fith wicket falling}
        \label{alldens5fow}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[scale=0.28]{figures/fow5densFull.png}
        \caption{Overall density plot for FOW 5}
        \label{ovrdens5fow}
    \end{minipage}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[scale=0.28]{figures/fow9density.png}
        \caption{Density of all teams for ninth wicket falling}
        \label{alldens9fow}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[scale=0.28]{figures/fow9densfull.png}
        \caption{Overall density plot for FOW 9}
        \label{ovrdens9fow}
    \end{minipage}
\end{figure}

In Figure~\ref{ovrdens1fow}, we see the density is heavily skewed to the left. This makes sense, as the bowling team will presumably be starting 
their innings by using their best bowlers, who will be hunting to get wickets early on. In Figure~\ref{ovrdens5fow}, we see a much more normally distributed
density function. But in actual fact, we see this interesting second, smaller peak appearing lower down in the score. Does this make sense? It's certainly 
not suprising. What these two peaks exemplify is the fact games can go heavily in favour of the bowling team, which can be seen in the first small peak,
wherein they have taken a lot of wickets in quick succession, meaning the lower order batters are coming in to bat earlier than usual. Secondly, it shows when the 
batting team is having a good day, because we have this much larger peak around the 200 runs mark.\\

Finally, in Figure~\ref{ovrdens9fow}, we can see that the earlier bowling advantage peak is much higer, because the lower order batters are traditionally less skilled 
at batting, and so the bowling team have a distinct advantage in taking wickets against these players. But we also see the second, batting-favoured peak is no much lower.
This corresponds to the scenario in which the earlier batters have laid a good foundation of the game, and the lower-order batters have not had to contribute much to the score.

\section{Outliers in Runs Data}
\label{mse}
In the next chapter, it will be necessary to choose a loss function for improving the neural network that we create. To aid in determining which function to use, we need to look at the
spread of runs scored in a full innings of data. This can be seen in Figure~\ref{runsbox}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{figures/runsbox.png}
    \caption{Boxplot showing the spread of runs scored}
    \label{runsbox}
\end{figure}

There are 363 data points greater than the third quartile, while 671 are below the first quartile. So $25.3\%$ of our data lies above the third quartile, and
$46.7\%$ below the first. For that reason, we make the decision to use the Mean-Squared-Error (MSE) loss function, which is commonplace in regression problems.
This follows from the fact that the MSE loss function is sensitive to outliers, and will weight the outliers appropriately among the dataset. In some applications 
this would be a disadvantage, however since our dataset is bounded (i.e, only 20 games have ever scored above 400, and none have gone above 500), the squared error 
is not likely to exceed a large amount, thus giving it a dissproportionate weighting. If in a few years scores start exceeding 500 or even 600, it would likely 
be appropriate to look at using a different loss function, but the MSE works well here for the afforementioned reasons.

\section{On the Normality of Run Totals}
It will be useful in later parts of this dissertation to know whether or not scores are normally distributed. To test whether or not 
they are, we use a Q-Q plot to check. This is a graphical way for checking normality, by comparing the quantiles of a dataset (in this case, scores) to quantiles
drawn from a theoretical distribution. If the resulting points follow a straight line, it is likely that the data came from the distribution.
In this case, we use the R function \textit{qqnorm()} to test if the runs from the 1436 games of a full 50 overs follow a normal distribution. See Appendix A for 
a more detailed discussion of this method. \\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{figures/qqnormplot.png}
    \caption{Q-Q plot for runs scored after 50 overs.}
    \label{qqplot50}
\end{figure}

We can see from Figure~\ref{qqplot50} that the plot follows along the straight line, and so we can conclude that the scores are infact normally distributed. Note however 
in the top right we see normality starts to trail off sligthly. As previously mentioned, only 20 games have gone above a socre of 400 in ODI cricket, and so 
there is a distinct lack of data in this range. That is why the curve trails off slightly. 
Further, we can estimate the parameters of this distribution using the R functions \textit{mean()} and \textit{var()}. Doing so gives that the distribution
of scores in 50 overs, $S_{50}$, can be given as $S_{50} \sim \mathcal{N}(270.56,51.34^2)$. \\ 

To further test that this is indeed the case, we can create a sample plot based on this distribution, which can be seen in Figure~\ref{samplenorm}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{figures/samplenorm.png}
    \caption{Sample plot created from the derived distribution of $S_{50}$}
    \label{samplenorm}
\end{figure}

With this in mind, we can now look at a density plot for the actual data. This is given in Figure~\ref{errDistAndQQRunsScored} (a).

\begin{figure}[h]
    \centering
    \subfloat[\centering Density for Runs Scored]{{\includegraphics[width=.4\linewidth]{figures/runsdensity.png} }}
    \qquad
    \subfloat[\centering Q-Q Plot for First Innings]{{\includegraphics[width=.4\linewidth]{figures/firstInnsQQ.png} }}
    \caption{Error Density and QQ for 50-over Data}
    \label{errDistAndQQRunsScored}
\end{figure}

It is unsuprising that runs are normally distributed, but to be able to draw a mean and variance from this will be very helpful in future aspects of the project.

We have seen that first innings scores in a full 50 overs are normally distributed. We can check, using the same methods if runs in a first innings that 
doesn't necessarily go for the full allowance of overs is normally distributed. 

We find that $S_{FI} \sim \mathcal{N}(213.49,56.91^2)$. 

\section{Run Rates}
\label{exprr}

\subsection{General Exploration}
The work that follows in this section is essential for allowing the Neural Network constructed in Chapter 5 to predict the scores of games. The aim of this section is 
to see if we can draw the run rates at specific overs from a statistical distribution. This will in turn enable us to fill the gaps in the missing overs of a game. In turn, 
with the gaps filled, we can pass the simmulated run rates to the neural network, and allow for a score to be predicted. \\

\begin{figure}[h]
    \centering
    \subfloat[\centering Average run rate per over]{{\includegraphics[width=.4\linewidth]{figures/avgRR.png} }}
    \qquad
    \subfloat[\centering Run rate Standard deviation per over]{{\includegraphics[width=.4\linewidth]{figures/sdrr.png} }}
    \caption{Error distribution with Q-Q plot}
    \label{errDistQQ3}
\end{figure}
\label{errdistqq3}
We can see from Figure~\ref{errDistQQ3}, that average Run Rate seems to stay consistent in the middle overs, before rapidly increasing as the risk associated with losing 
a wicket falls off due to the end of the innings coming closer. If we break the game into five ten-over segments, as these are generally different periods of the game
from a tactical perspective\footnote{This is to do with different bowling options being used to be more effective as the pitch conditions change}, we can model the segments.
A game of ODI-cricket can roughly be broken up to three segments, the first 10 overs, known as the ``powerplay'', where teams are slightly more conservative and look to build a 
foundation on which the rest of the game is built. The middle overs are 11-35. The idea of this game is to continue building on the foundation, and save wickets. The last 
15 overs are where more risks are taken, as the value of a wicket decreases. The rule of thumb for the batting team is to double your score from the first 35 in the last 15, which is why we see the massive 
increase in run rate in these last overs. \\

Using this information, we can create density plots of the average Run Rate in these overs, as in figures 4.13-4.15. 

\begin{figure}
    \minipage{0.32\textwidth}
      \includegraphics[width=\linewidth]{figures/powerplaydens.png}
      \caption{Powerplay Run Rate density}
    \endminipage\hfill
    \minipage{0.32\textwidth}
      \includegraphics[width=\linewidth]{figures/middleoversdens.png}
      \caption{Middle Overs Run Rate density}
    \endminipage\hfill
    \minipage{0.32\textwidth}%
      \includegraphics[width=\linewidth]{figures/finaloversdens.png}
      \caption{Final Overs density}
    \endminipage
    \label{rrDensitiesPlot}
\end{figure}

The run rates are  in different stages of the game roughly normally distributed, which can be seen in Figure 4.12-4.14. It is reasonable that due to the Central Limit Theorem, with more observations, these distributions would be smoothed out and 
appear more normally distributed than it does at present. This is important, because it allows us to investigate using a monte-carlo method to fill in gaps 
later on in the project.

\subsection{Feature Selection}
\label{lassoSec}

Feature selection is an important aspect in data science. Some experiments can have an overwhelming number of variables, which can lead to computational inefficiency. In this subsection, we will explore selecting the most important overs in the run rate data.
Given the lack of data already available, this may seem unnecessary, however it is an interesting consideration for future work. When we build the network in the next chapter, the model has to be passed a formula, telling it how to treat the variables. By default, since we have 50 overs worth of data (V1 $\rightarrow$ V50) and a variable containing the final score, V51, we pass the R formula $V51 \sim V1 + \ldots + V50$. The purpose of this section is to see if it is worth reducing this somewhat combersome formula. 
LASSO, \textbf{L}east \textbf{A}bsolute \textbf{S}hrinkage and \textbf{S}election \textbf{O}perator, is a regression method for performing both normalisation, and feature selection. The term LASSO was introduced by \cite{tib}. Before showing how we implement this in R, and what it means for our project, it is useful to discuss the mathematical foundation of the method, as in section 2.1 of \cite{tib}. \\

For $i=1,\ldots,N$, we have a dataset $(\textbf{X}^i,y_i)$. For the training dataset we use, $N=1436$. Specifically, we define $\textbf{X}^i = (x_{i1},\ldots,x_{ip})$ to be the vector of predictor variables (for us, $p=50$). Further, $y_i$ is the response variable. The assumption that these variables are independent is immediately satistfied by the nature of our study, since the runs scored in one over does not depend at all on the runs scored in the previous over. This is because a lot changes from over-to-over and even ball-to-ball;
for example, the fielding positions change, the bowler will never bowl the ball exactly on the same spot twice in a row and the ends from which the balls are being bowled changes.  
In addition, our data is normalised when it is passed the neural network, and so the normalisation assumption is also satisfied by default. 

\begin{definition}
    We begin by letting $\beta = (\hat{\beta_1},\ldots,\hat{\beta_p})^T$, then the \textbf{LASSO estimate} $(\hat{\alpha}, \hat{\beta})$, for a tuning parameter $t \geq 0$ is given by
    \[
        (\hat{\alpha},\hat{\beta}) = \text{argmin} \left\{ \sum_{i=1}^N \left(y_i - \alpha - \sum_j \beta_j x_{ij} \right)^2 \right\} \\
    \]
    Suject to $\sum_j |\beta_j| \leq t$. 
\end{definition}

We have that $\forall t$, $\hat{a} = \bar{y}$. However since the data is normalised, by defintion $\bar{y}=0$ and so we can ignore the parameter $\alpha$ altogether. Although not discussed directly here, the authors do go on to propose algorithms for computing solutions to this problem in chapter 6 of their paper. These algorithms are employed directly by the R package \verb|glmnet|, which we can now use. \\

We use the function \textit{cv.glmnet()}. This can be seen in Figure~\ref{lassoCode}.

\begin{figure}[h]
    \lstinputlisting[language=R, firstline=6, lastline=7]{../Code/Rscripts/lasso.R}
    \caption{Fitting a LASSO model}
    \label{lassoCode}
\end{figure}

The plot produced by this code can be seen in Figure~\ref{lassoFig}. Note the numbers at the top are the overs- although now relabeled from 0 $\to$ 49. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/lasso.png}
    \caption{LASSO Model Plot}
    \label{lassoFig}
\end{figure}

The red dots here show the mean absolute error when the number of variables used in the model. We can see from this plot that using all of the overs results in the lowest MAE. Seeing which variables contribute the most can be done using the code in Figure~\ref{lassoVals}. 

\begin{figure}[h]
    \lstinputlisting[language=R, firstline=12, lastline=13]{../Code/Rscripts/lasso.R}
    \caption{Contributing Variables}
    \label{lassoVals}
\end{figure}

This code orders the variables depending on how ``important'' they are. This ordering is done by the minimum lambda value. As it turns out, the 5 most important overs are 31, 46, 35, 38 and 48. Unsuprisingly, the least important is over 1. This is more than understandable from a cricketing standpoint, since a lot can change after the first over, whereas the later overs can make or break an innings. 

